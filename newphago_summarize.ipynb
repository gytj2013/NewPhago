{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "newphago_summarize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gytj2013/NewPhago/blob/main/newphago_summarize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ],
      "metadata": {
        "id": "lt_rPZVTgC39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu7GbJf4m4h1"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy\n",
        "!pip install git+https://github.com/kakaobrain/pororo.git\n",
        "!pip install pykrx \n",
        "!pip install jamo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "metadata": {
        "id": "9tybPTd5LfeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Mecab-ko-for-Google-Colab"
      ],
      "metadata": {
        "id": "2dPvs2oTL3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash install_mecab-ko_on_colab_light_220111.sh"
      ],
      "metadata": {
        "id": "wKyMiEvtL8rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pororo import Pororo"
      ],
      "metadata": {
        "id": "9tQ0AINXo6KL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "id": "pqp8GQ7OPbX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Finance-DataReader"
      ],
      "metadata": {
        "id": "XCLBPryoiZgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코스피 코스닥 상장 기업 정보 리스트 생성\n",
        "from pykrx import stock\n",
        "import pandas as pd\n",
        "import FinanceDataReader as fdr\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "today = datetime.today().strftime(\"%Y%m%d\")\n",
        "print(today)\n",
        "last_week = (datetime.today() - timedelta(7)).strftime(\"%Y%m%d\")\n",
        "print(last_week)\n",
        "\n",
        "KOSDAQ = stock.get_market_ticker_list(market=\"KOSDAQ\")\n",
        "KOSPI = stock.get_market_ticker_list(market=\"KOSPI\")\n",
        "\n",
        "def return_name():\n",
        "    Company = []\n",
        "\n",
        "    for ticker in KOSDAQ:\n",
        "        Value =stock.get_market_ticker_name(ticker)\n",
        "        Company.append([Value,ticker,\"KOSDAQ\"]) # 기업명, 종목번호, 코스닥 리스트에 추가\n",
        "      \n",
        "    for ticker in KOSPI:\n",
        "        Value =stock.get_market_ticker_name(ticker)\n",
        "        Company.append([Value,ticker,\"KOSPI\"]) # 기업명, 종목번호, 코스닥 리스트에 추가\n",
        " \n",
        "    return Company;"
      ],
      "metadata": {
        "id": "bhLnQVi9p1AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 종성 여부 체크\n",
        "from jamo import h2j, j2hcj\n",
        "\n",
        "def get_jongsung_TF(text):\n",
        "  sample_text_list = list(text)\n",
        "  last_word = sample_text_list[-1]\n",
        "  last_word_jamo_list = list(j2hcj(h2j(last_word)))\n",
        "  last_jamo = last_word_jamo_list[-1]\n",
        "\n",
        "  jongsung_TF = \"T\"\n",
        "\n",
        "  if last_jamo in ['ㅏ','ㅑ','ㅓ','ㅕ','ㅗ','ㅛ','ㅜ','ㅠ','ㅡ','ㅣ','ㅘ','ㅚ','ㅙ','ㅝ','ㅞ','ㅢ','ㅐ','ㅔ','ㅟ','ㅖ','ㅒ'] :\n",
        "    jongsung_TF = \"F\"\n",
        "\n",
        "  return jongsung_TF"
      ],
      "metadata": {
        "id": "KgBKosMxSweY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 사전 적용 : 고유명사 구분하기 위해 코스피, 코스닥 상장 기업들 사용자 사전에 추가\n",
        "with open(\"./mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\",'r',encoding='utf-8') as f:\n",
        "  file_data = f.readlines()\n",
        "Company = return_name()\n",
        "for c in Company:\n",
        "  jongsung_TF = get_jongsung_TF(c[0])\n",
        "  line = '{},,,,NNP,*,{},{},*,*,*,*,*\\n'.format(c[0], jongsung_TF, c[0])\n",
        "  file_data.append(line)\n",
        "with open(\"./mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\",'w',encoding='utf-8') as f:\n",
        "  for line in file_data:\n",
        "    f.write(line)"
      ],
      "metadata": {
        "id": "-6aDOj0xRHkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스 요약 및 기업 정보 추출\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Mecab\n",
        "from pykrx import stock\n",
        "import pandas as pd\n",
        "import FinanceDataReader as fdr\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "today = datetime.today().strftime(\"%Y%m%d\")\n",
        "last_week = (datetime.today() - timedelta(7)).strftime(\"%Y%m%d\")\n",
        "\n",
        "bullet_summ = Pororo(task=\"text_summarization\", lang=\"ko\", model=\"bullet\")\n",
        "abs_summ = Pororo(task=\"text_summarization\", lang=\"ko\", model=\"abstractive\")\n",
        "\n",
        "def summerize_news(url, Company):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"}\n",
        "    req = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    body = soup.find_all(id='dic_area')\n",
        "    if len(body) > 0:\n",
        "        text = body[0].text\n",
        "        output_bullet = bullet_summ(text,\n",
        "                                    beam=5,\n",
        "                                    len_penalty=0.6,\n",
        "                                    no_repeat_ngram_size=3,\n",
        "                                    top_k=50,\n",
        "                                    top_p=0.7)\n",
        "        summary_output = abs_summ(text,\n",
        "                                  beam=5,\n",
        "                                  len_penalty=0.6,\n",
        "                                  no_repeat_ngram_size=3,\n",
        "                                  top_k=50,\n",
        "                                  top_p=0.7\n",
        "                                  )\n",
        "      \n",
        "        mecab = Mecab()\n",
        "        noun = mecab.nouns(summary_output) # 명사만 추출\n",
        "        pos = mecab.pos(summary_output) # 형태소 분석\n",
        "        removelist = ['JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC', 'EF'] # 조사 품사 리스트 - 제거할 품사\n",
        "        resultlist = []\n",
        "        for p in pos :\n",
        "          if len(p[0])>1 and p[1] not in removelist : # 토큰의 길이가 1 초과이고 조사가 아니면 감정 분석에 활용할 토큰이므로 리스트에 추가\n",
        "            resultlist.append(p)\n",
        "        #print(pos)\n",
        "        #print(resultlist)\n",
        "\n",
        "        company_name = []\n",
        "        for c in Company :\n",
        "          company_name.append(c[0]);\n",
        "\n",
        "        summ = \"[주요] \\n\"\n",
        "        for line in output_bullet:\n",
        "          summ += \"-\"+line+\"\\n\"\n",
        "        summ += \"[요약] \\n\"\n",
        "        summ += summary_output+\"\\n\" # 뉴스 요약본\n",
        "        summ += \"[회사정보] \\n\"\n",
        "        chk = False\n",
        "        for n in noun : # 뉴스 요약본 속 명사들 중에서 코스피, 코스닥 상장 기업명 찾기\n",
        "          try :\n",
        "            idx = company_name.index(n)\n",
        "          except :\n",
        "            chk = False\n",
        "          else :\n",
        "            summ += str(Company[idx])\n",
        "            chk = True\n",
        "            break\n",
        "        if chk==False :\n",
        "          summ += \"null \\n\"\n",
        "        print(summ)\n",
        "        ticker = Company[idx][1]\n",
        "        info_a = stock.get_market_ohlcv(last_week, today, ticker)\n",
        "        print(info_a) #주가\n",
        "        info_b = stock.get_market_cap(last_week, today, ticker)\n",
        "        print(info_b) #시가총액, 거래량, 거래대금, 상장주식수 \n",
        "        info_c = stock.get_exhaustion_rates_of_foreign_investment(last_week, today, ticker)\n",
        "        print(info_c) #외국인 보유량 및 외국인 한도소진율\n",
        "        info_d = stock.get_market_fundamental(last_week, today, ticker)\n",
        "        print(info_d) #DIV/BPS/PER/EPS\n",
        "        info_e = stock.get_shorting_volume_by_date(last_week, today, ticker)\n",
        "        print(info_e) #공매도 현황\n",
        "        info_g = fdr.DataReader(symbol=ticker, start= \"2020\")\n",
        "        fig = plt.figure(figsize=(14, 6))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        ax.plot(info_g['Close'])\n",
        "        ax.set_xlabel(\"Date\")\n",
        "        ax.set_ylabel(ticker)\n",
        "        ax.set_title(\"stock price\")\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.show() #주가그래프\n",
        "        return summ\n",
        "    else:\n",
        "        print(\"error\")\n",
        "        return \"error\""
      ],
      "metadata": {
        "id": "L3e2MBeqx8b8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import re\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# 오늘 날짜 형식화\n",
        "now=datetime.now()\n",
        "now=str(now)\n",
        "year=now[0:4]\n",
        "month=now[5:7]\n",
        "day=now[8:10]\n",
        "res=year+month+day\n",
        "\n",
        "# url file에 저장 (max_page_num 바꿔주면 됨)\n",
        "output_file_name=str(res)+\"_url.txt\"\n",
        "output_file = open(output_file_name, \"w\", encoding=\"utf-8\")\n",
        "page_num = 1\n",
        "max_page_num = 1\n",
        "\n",
        "user_agent = \"'Mozilla/5.0\"\n",
        "headers ={\"User-Agent\" : user_agent}\n",
        "\n",
        "urls = []\n",
        "while page_num<=max_page_num:\n",
        "    page_url = \"http://news.naver.com/main/list.nhn?sid2=258&sid1=101&mid=shm&mode=LS2D&date=\" + str(res) + \"&page=\" + str(page_num) + \"\"\n",
        "    response = requests.get(page_url, headers=headers)\n",
        "    html = response.text\n",
        "    # url 추출\n",
        "    url_frags = re.findall('<a href=\"(.*?)\"',html)\n",
        "    for url_frag in url_frags:\n",
        "        if \"article\" in url_frag:\n",
        "            urls.append(url_frag)\n",
        "            #print(url_frag)\n",
        "    page_num+=1\n",
        "urls=set(urls)\n",
        "for url in urls:\n",
        "    print(url, file=output_file)\n",
        "time.sleep(2)\n",
        "output_file.close()\n",
        "\n",
        "# url 바탕으로 요약문 생성\n",
        "# 요약문 file에 저장(테스트용으로 count로 6개만 끊어준것임)\n",
        "output_file_name2=str(res)+\"_summarized.txt\"\n",
        "output_file2 = open(output_file_name2, \"w\", encoding=\"utf-8\")\n",
        "count=0\n",
        "for url in urls:\n",
        "  if(count>20):\n",
        "    break\n",
        "  count=count+1\n",
        "  print(summerize_news(url,return_name()), file=output_file2)\n",
        "output_file2.close()\n",
        "'''\n",
        "# mount it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# copy it there\n",
        "!cp \"20220408_summarized.txt\" /content/drive/MyDrive\n",
        "!cp \"20220408_url.txt\" /content/drive/MyDrive\n",
        "'''\n"
      ],
      "metadata": {
        "id": "2ctsAa2Apzer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "#summerize_news('https://n.news.naver.com/mnews/article/023/0003688236?sid=101', return_name())"
      ],
      "metadata": {
        "id": "a67ezLSK0zma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}